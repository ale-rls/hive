{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN+CLIP_with_Latent_Bootstraping_Public.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "NpQlC-FOh6tj",
        "GTI8SzEaUUIs",
        "7JytSQK_bh19"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/main/interesting_notebooks/StyleGAN%2BCLIP_with_Latent_Bootstraping_Public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf_m82oAd4aL",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Eric Hallahan\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpQlC-FOh6tj"
      },
      "source": [
        "# First Run Setup\n",
        "**Restart after run.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Wj6Aan21xH"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import subprocess\n",
        "import contextlib\n",
        "\n",
        "with contextlib.redirect_stdout(io.StringIO()) as f:\n",
        "    %pip show torch\n",
        "torch_string = list(filter(lambda x: x.__contains__('Version: '),f.getvalue().splitlines()))[0]\n",
        "torch_version = torch_string.split()[1]\n",
        "torch_version_suffix = ''.join(torch_version.partition('+')[1:])\n",
        "\n",
        "with contextlib.redirect_stdout(io.StringIO()) as f:\n",
        "    %pip show torch\n",
        "torchvision_string = list(filter(lambda x: x.__contains__('Version: '),f.getvalue().splitlines()))[0]\n",
        "torchvision_version = torch_string.split()[1]\n",
        "torchvision_version_suffix = ''.join(torch_version.partition('+')[1:])\n",
        "\n",
        "CUDA_toolkit_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA Toolkit Version:\", CUDA_toolkit_version)\n",
        "\n",
        "if torch_version_suffix=='':\n",
        "    torch_version_suffix = f'+cu{int(float(CUDA_version)*10)}'\n",
        "    if not os.path.exists('/dev/nvidia0'):\n",
        "        torch_version_suffix = \"+cpu\"\n",
        "\n",
        "print(torch_version_suffix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-pOqsk27YY"
      },
      "source": [
        "%pip install --upgrade numpy torch==1.10.0{torch_version_suffix} torchvision==0.11.1{torch_version_suffix} torchtext -f https://download.pytorch.org/whl/torch_stable.html ftfy regex git+https://github.com/openai/CLIP.git ninja git+https://github.com/geoopt/geoopt.git exrex scipy\n",
        "%pip cache purge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCHx8RJDcQjW"
      },
      "source": [
        "!git clone https://github.com/NVlabs/stylegan3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDemDnliayrp"
      },
      "source": [
        "%%writefile clip_ensemble_ext.py\n",
        "# This is a cursed monkeypatch designed to enable simple ensembling of CLIP models.\n",
        "# After importing `clip` and this package, multiple model names may be passed to\n",
        "# `clip.load()` as a list.\n",
        "from functools import singledispatch\n",
        "import torch\n",
        "import torchvision.transforms\n",
        "import clip\n",
        "\n",
        "clip.load = singledispatch(clip.load)\n",
        "\n",
        "class CLIPEnsemble(torch.nn.Module):\n",
        "    def __init__(self, models):\n",
        "        super().__init__()\n",
        "        self.input_resolutions = set()\n",
        "        class Visual():\n",
        "            def __init__(self):\n",
        "                self.output_dim = 0\n",
        "                self.input_resolution = 0\n",
        "        self.visual = Visual()\n",
        "        for model in models:\n",
        "            self.add_module(model[0], model[1])\n",
        "            self.input_resolutions |= set([model[1].visual.input_resolution])\n",
        "            self.visual.output_dim += model[1].visual.output_dim\n",
        "        self.num_models = len(models)\n",
        "        self.input_resolutions = sorted(self.input_resolutions)\n",
        "        self.visual.input_resolution = self.input_resolutions[-1]\n",
        "    \n",
        "    def encode_image(self, image):\n",
        "        def normalize(x):\n",
        "            return (x.transpose(0,1)/torch.norm(x,dim=-1)).transpose(0,1)\n",
        "        if image.dim() != 5:\n",
        "            def preprocess(image):\n",
        "                def pad(image):\n",
        "                    max_res = self.visual.input_resolution\n",
        "                    assert image.shape[-1]==image.shape[-2]\n",
        "                    padding = max_res-image.shape[-1]\n",
        "                    return torchvision.transforms.functional.pad(image,[0,0,padding,padding])\n",
        "                return torch.stack([pad(torchvision.transforms.functional.resize(image,n_px)) for n_px in self.input_resolutions],dim=1)\n",
        "            image = preprocess(image)\n",
        "        return normalize(torch.cat([normalize(module.encode_image(image[:,self.input_resolutions.index(module.visual.input_resolution),:,:module.visual.input_resolution,:module.visual.input_resolution])) for module in self.children()],dim=-1))\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        def normalize(x):\n",
        "            return x/torch.norm(x,dim=-1)\n",
        "        return normalize(torch.cat([normalize(module.encode_text(text)) for module in self.children()],dim=-1))\n",
        "\n",
        "def ensemble_load(model_names):\n",
        "    model_names = sorted(list(set(model_names)),key=lambda i: clip.available_models().index(i))\n",
        "    model = CLIPEnsemble([(name,clip.load(name)[0]) for name in model_names])\n",
        "    _preprocess = [clip.clip._transform(n_px) for n_px in model.input_resolutions]\n",
        "    def preprocess(image):\n",
        "        def pad(image):\n",
        "            max_res = model.visual.input_resolution\n",
        "            assert image.shape[-1]==image.shape[-2]\n",
        "            padding = max_res-image.shape[-1]\n",
        "            return torchvision.transforms.functional.pad(image,[0,0,padding,padding])\n",
        "        return torch.stack([pad(_transform(image)) for _transform in _preprocess],dim=0)\n",
        "    return model, preprocess\n",
        "\n",
        "clip.load.register(list,ensemble_load)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1itOgxMarijf"
      },
      "source": [
        "# General Setup\n",
        "*   Import StyleGAN and CLIP\n",
        "*   Read in the index\n",
        "*   Generate the vectors in $z$ that corespond to the index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAO20Bw4BSUi",
        "cellView": "form"
      },
      "source": [
        "import os\n",
        "import contextlib\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "import stylegan3.torch_utils as torch_utils\n",
        "import stylegan3.dnnlib as dnnlib\n",
        "%cd stylegan3/\n",
        "from gen_images import make_transform\n",
        "%cd ..\n",
        "import clip\n",
        "import clip_ensemble_ext #TODO: Fully integrate ensemble support\n",
        "import exrex\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torchtext.utils import download_from_url\n",
        "\n",
        "#@markdown StyleGAN network `.pkl` file (either PyTorch or legacy TensorFlow format) or API model name:\n",
        "network_pkl = \"stylegan2-ffhq-1024x1024\" #@param {type:\"string\"}\n",
        "pkl_basename = os.path.basename(network_pkl)\n",
        "network_pkl = f\"https://api.ngc.nvidia.com/v2/models/nvidia/research/{pkl_basename.split('-')[0]}/versions/1/files/{network_pkl}.pkl\" if network_pkl[-4:]!=\".pkl\" else network_pkl \n",
        "\n",
        "#@markdown CLIP model:\n",
        "clip_model = \"ViT-B/32\" #@param [\"ViT-B/32\", \"ViT-B/16\", \"RN50x16\"]\n",
        "\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if cuda_available else 'cpu')\n",
        "\n",
        "# Load StyleGAN\n",
        "with dnnlib.util.open_url(network_pkl) as f:\n",
        "    # If legacy pkl then convert before loading. \n",
        "    try:\n",
        "        G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module\n",
        "    except ModuleNotFoundError:\n",
        "        import legacy\n",
        "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
        "\n",
        "if hasattr(G.synthesis, 'input'):\n",
        "    m = make_transform((0,0), 0)\n",
        "    m = np.linalg.inv(m)\n",
        "    G.synthesis.input.transform.copy_(torch.from_numpy(m))\n",
        "    shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n",
        "    G.synthesis.input.affine.bias.data=shift.squeeze(0)\n",
        "    G.synthesis.input.affine.weight.data.zero_()\n",
        "\n",
        "# Load CLIP\n",
        "model, preprocess = clip.load(clip_model)\n",
        "\n",
        "# Download the selected index if it exists\n",
        "indices = {\"stylegan2-ffhq-1024x1024_vit-b-32\":\n",
        "           {\"url\": \"https://drive.google.com/uc?export=download&id=1R2Ra6Bf7IKwM2eZMKwFvyjkWRyOulQaP\",\n",
        "            \"sha256\": \"249219a1ad4a37c7b7d72995208bd6d812b839bcd2db3424384ca69d4fdee718\"}, \n",
        "           \"stylegan3-r-ffhqu-256x256_rn50x16\":\n",
        "           {\"url\": \"https://drive.google.com/uc?export=download&id=1YkZuU4mF6QI38v-HPleMhNnsXLDQ7N_d\",\n",
        "            \"sha256\": \"6fa85a4a889013c46c99fb75c0fcd677c28c1a42a7019a844dd2f513c7e60066\"},\n",
        "           \"stylegan3-r-ffhqu-1024x1024_vit-b-32\":\n",
        "           {\"url\": \"https://drive.google.com/uc?export=download&id=1o8mtX78vJgP1bCmINMCZFnmdDLbhW6XB\",\n",
        "            \"sha256\": \"163da34a641f30ed1672727f338f99adbfe7a7ec0bdf3e491dadd301757be1cf\"}}\n",
        "index_name = f\"{os.path.splitext(pkl_basename)[0]}_{clip_model.replace('/','-').lower()}\"\n",
        "index_path = f\"{os.path.curdir}{os.path.sep}indices{os.path.sep}{index_name}.npy\"\n",
        "\n",
        "try:\n",
        "    download_from_url(indices[index_name][\"url\"], path=index_path, hash_value=indices[index_name][\"sha256\"])\n",
        "except (RuntimeError, KeyError) as exception:\n",
        "    if isinstance(exception,RuntimeError):\n",
        "        # Fallback upon gdown when we encounter https://github.com/pytorch/text/issues/1359\n",
        "        import gdown\n",
        "        gdown.download(indices[index_name][\"url\"], index_path, quiet=False)\n",
        "    if isinstance(exception,KeyError):\n",
        "        print(\"Index not found, skipping...\")\n",
        "    \n",
        "# Load the index into memory if it exists\n",
        "if os.path.exists(index_path):\n",
        "    CLIP_vecs = torch.from_numpy(np.load(index_path))\n",
        "    seeded_z = torch.from_numpy(np.stack([np.random.RandomState(seed).randn(G.w_dim) for seed in range(CLIP_vecs.shape[0])]))\n",
        "\n",
        "def sample_index(features, mode='w'):\n",
        "    tmp = torch.nn.functional.cosine_similarity(CLIP_vecs,features.cpu())\n",
        "    tmp, indexes = torch.topk(tmp,k,dim=0)\n",
        "    tmp = torch.softmax(tmp/0.01,dim=-1)\n",
        "    if mode=='w':\n",
        "        ws = G.mapping((seeded_z[indexes]).reshape(-1,G.w_dim).to(device), c=None).cpu()\n",
        "        found_w = torch.sum(ws*tmp.unsqueeze(1).unsqueeze(2),dim=0).unsqueeze(0)\n",
        "    if mode=='z':\n",
        "        found_w = G.mapping(spherical_avg(seeded_z[indexes],w=tmp).unsqueeze(0).to(device), c=None).cpu()\n",
        "    return found_w\n",
        "\n",
        "# Adapted preprocessing for connecting StyleGAN to CLIP \n",
        "def _stylegan_transform(n_px):\n",
        "    return Compose([\n",
        "        Resize((n_px,n_px)),\n",
        "        lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "_stylegan_preprocess = [_stylegan_transform(n_px) for n_px in model.input_resolutions] if hasattr(model,'input_resolutions') else _stylegan_transform(model.visual.input_resolution)\n",
        "\n",
        "def stylegan_preprocess(image):\n",
        "    def pad(image):\n",
        "        import torchvision.transforms\n",
        "        max_res = max(model.input_resolutions)\n",
        "        assert image.shape[-1]==image.shape[-2]\n",
        "        padding = max_res-image.shape[-1]\n",
        "        return torchvision.transforms.functional.pad(image,[0,0,padding,padding])\n",
        "    return torch.cat([pad(_transform(image)) for _transform in _stylegan_preprocess],dim=0) if type(_stylegan_preprocess) is list else _stylegan_preprocess(image)\n",
        "\n",
        "# Prompt Aug Utilities\n",
        "def spherical_avg(p, w=None, tol=1e-6):\n",
        "    \"\"\"Applies a weighted spherical average as described in the paper \n",
        "    `Spherical Averages and Applications to Spherical Splines and \n",
        "    Interpolation <http://math.ucsd.edu/~sbuss/ResearchWeb/spheremean>`__ .\n",
        "    \n",
        "    Args:\n",
        "        p (torch.Tensor): Input vectors\n",
        "        w (torch.Tensor, optional): Weights for averaging.\n",
        "        tol (float, optional): The desired tolerance of the output. \n",
        "            Default: 1e-6\n",
        "    \"\"\"\n",
        "    from geoopt import Sphere\n",
        "    sphere = Sphere()\n",
        "    if w is None:\n",
        "        w = p.new_ones([p.shape[0]])\n",
        "    assert p.ndim == 2 and w.ndim == 1 and len(p) == len(w)\n",
        "    w = w / w.sum()\n",
        "    p = sphere.projx(p)\n",
        "    q = sphere.projx(p.mul(w.unsqueeze(1)).sum(dim=0))\n",
        "    for i in range(1000):\n",
        "        q_new = sphere.retr(q, sphere.logmap(q, p).mul(w.unsqueeze(1)).sum(dim=0))\n",
        "        norm = torch.linalg.vector_norm(q.sub(q_new))\n",
        "        q = q_new\n",
        "        if norm <= tol:\n",
        "            break\n",
        "    return q\n",
        "\n",
        "try:\n",
        "    os.mkdir(f\"{os.curdir}{os.path.sep}prompt_images\")\n",
        "except FileExistsError:\n",
        "    pass\n",
        "    \n",
        "pattern = re.compile(r\"(<(?P<weight>-{0,1}[0-9]*\\.*[0-9]*)>){0,1}(?P<prompt>.*)\")\n",
        "\n",
        "class AugmentedPrompt(str):\n",
        "    def __new__(cls, *args, **kwargs):\n",
        "        return super().__new__(cls, args[0])\n",
        "    \n",
        "    def __init__(self, *args, weight=1, **kwargs):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "        self.negated = self.weight < 0\n",
        "        if 'filepath' in kwargs.keys():\n",
        "            if kwargs['filepath'] is not None:\n",
        "                self.filepath = kwargs['filepath']\n",
        "\n",
        "\n",
        "def aug_info(s):\n",
        "    \"\"\"\n",
        "    Construct an `AugmentedPrompt` from a plaintext string.\n",
        "    \"\"\"\n",
        "    match = re.match(pattern, s)\n",
        "    weight, prompt = (match.group('weight'), match.group('prompt'))\n",
        "    return AugmentedPrompt(prompt.replace('~',''), \n",
        "                           weight=(-1 if prompt.__contains__('~') else 1)*(float(weight) if weight is not None else 1), \n",
        "                           filepath=f\"{os.curdir}{os.path.sep}prompt_images{os.path.sep}{prompt.replace('~','').strip()}\" if os.path.isfile(f\"{os.curdir}{os.path.sep}prompt_images{os.path.sep}{prompt.replace('~','').strip()}\") else None)\n",
        "\n",
        "def prompt_parse(prompt,limit=32):\n",
        "    \"\"\"\n",
        "    Consume a string and return parsed augmented prompts and info.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Escape periods for convieance. \n",
        "        prompt = prompt.replace('\\\\.','<|escape_period|>').replace('.','\\\\.').replace('<|escape_period|>','.')\n",
        "        # Expand the regex to matching strings\n",
        "        augmented_prompts_raw = list(exrex.generate(prompt))\n",
        "        # Protect the user from generating too many strings\n",
        "        assert len(augmented_prompts_raw)<=limit\n",
        "        # Extract information from each prompt\n",
        "        return [aug_info(x) for x in augmented_prompts_raw]\n",
        "\n",
        "\n",
        "def prompt_preview(augmented_prompts):\n",
        "    \"\"\"\n",
        "    Format a sequence of `AugmentedPrompt` for a notebook.\n",
        "    \"\"\"\n",
        "    # Display the results of augmentation to the user, printing explicitly negative strings red\n",
        "    from IPython.display import display,HTML\n",
        "    formatted_prompts = ''.join([f\"<tr><td><pre style='color:red'>{x}</pre></td></tr>\" if x.negated else f\"<tr><td><pre>{x}</pre></td><t/r>\" for x in augmented_prompts])\n",
        "    display(HTML(data=\"<table style='border: 1px solid'><thead><tr><th>Prompt Preview</th></tr></thead><tbody>\"+formatted_prompts+\"</tbody></table>\"))\n",
        "\n",
        "def aug_prompts_to_features(augmented_prompts, condense=False):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        weights = torch.tensor([augmented_prompt.weight for augmented_prompt in augmented_prompts])\n",
        "        filepaths = [augmented_prompt.filepath for augmented_prompt in augmented_prompts if hasattr(augmented_prompt, 'filepath')]\n",
        "        filepath_mask = torch.tensor([hasattr(augmented_prompt, 'filepath') for augmented_prompt in augmented_prompts])\n",
        "\n",
        "        # Encode strings to features\n",
        "        features = torch.zeros(len(augmented_prompts),model.visual.output_dim)\n",
        "        if torch.any(torch.logical_not(filepath_mask)):\n",
        "            features[torch.logical_not(filepath_mask)] = model.encode_text(clip.tokenize(augmented_prompts)[torch.logical_not(filepath_mask)].to(device)).cpu().to(torch.float32)\n",
        "\n",
        "        if torch.any(filepath_mask):\n",
        "            if index_name.__contains__('ffhq'):\n",
        "                # Cache images so that we don't spend time aligning when we generate again.\n",
        "                new_filepaths = [os.path.join(os.path.dirname(filepath).replace('prompt_images','prompt_images_aligned'),sha256_file(filepath)+\".png\") for filepath in filepaths]\n",
        "                for filepath, new_filepath in zip(filepaths,new_filepaths):\n",
        "                    if not os.path.isfile(new_filepath):\n",
        "                        align_face(filepath).save(new_filepath)\n",
        "                filepaths = new_filepaths\n",
        "            features[filepath_mask] = model.encode_image(torch.stack([preprocess(PIL.Image.open(x)) for x in filepaths],dim=0).to(device)).cpu().to(torch.float32)\n",
        "\n",
        "        # Apply polarities\n",
        "        features = features*torch.sign(weights).unsqueeze(1)\n",
        "    \n",
        "        if condense:\n",
        "            features = condense_features(features, weights)\n",
        "\n",
        "    return features\n",
        "\n",
        "def condense_features(features, weights=None):\n",
        "    # If we have more than one feature vector use their spherical average instead\n",
        "    if features.shape[0]>1:\n",
        "        features = spherical_avg(features,w=torch.abs(weights) if weights is not None else weights).unsqueeze(0)\n",
        "    return features.squeeze()\n",
        "\n",
        "\n",
        "def sha256_file(filename):\n",
        "    import hashlib\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(filename,\"rb\") as f:\n",
        "        # Read and update hash string value in blocks of 4K\n",
        "        for byte_block in iter(lambda: f.read(4096),b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "# FFHQ Utilities\n",
        "if index_name.__contains__('ffhq'):\n",
        "    import sys\n",
        "    import os\n",
        "    import glob\n",
        "    import scipy\n",
        "    import scipy.ndimage\n",
        "    import dlib\n",
        "\n",
        "    try:\n",
        "        os.mkdir(f\"{os.curdir}{os.path.sep}prompt_images_aligned\")\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "    shape_predictor_path = download_from_url(\"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\",path=f\"{os.curdir}{os.path.sep}shape_predictor_68_face_landmarks.dat.bz2\",hash_value=\"7d6637b8f34ddb0c1363e09a4628acb34314019ec3566fd66b80c04dda6980f5\")[:-4]\n",
        "    import bz2\n",
        "    if not os.path.isfile(shape_predictor_path):\n",
        "        with open(shape_predictor_path, 'wb') as decompressed, bz2.BZ2File(shape_predictor_path+\".bz2\", 'rb') as compressed:\n",
        "            for data in iter(lambda : compressed.read(100 * 1024), b''):\n",
        "                decompressed.write(data)\n",
        "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
        "\n",
        "\n",
        "    def align_face(filepath,output_size=1024,transform_size=4096,enable_padding=True):\n",
        "        \"\"\"\n",
        "        :param filepath: str\n",
        "        :return: PIL Image\n",
        "        \"\"\"\n",
        "\n",
        "        def get_landmark(filepath):\n",
        "            \"\"\"get landmark with dlib\n",
        "            :return: np.array shape=(68, 2)\n",
        "            \"\"\"\n",
        "            detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "            img = dlib.load_rgb_image(filepath)\n",
        "            #TODO: Adapt this routine to handle multiple faces.\n",
        "            dets = detector(img, 1)\n",
        "\n",
        "            #print(\"Number of faces detected: {}\".format(len(dets)))\n",
        "            for k, d in enumerate(dets):\n",
        "                #print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
        "                #    k, d.left(), d.top(), d.right(), d.bottom()))\n",
        "                # Get the landmarks/parts for the face in box d.\n",
        "                shape = predictor(img, d)\n",
        "                #print(\"Part 0: {}, Part 1: {} ...\".format(shape.part(0), shape.part(1)))\n",
        "            \n",
        "            t = list(shape.parts())\n",
        "            a = []\n",
        "            for tt in t:\n",
        "                a.append([tt.x, tt.y])\n",
        "            lm = np.array(a)\n",
        "            # lm is a shape=(68,2) np.array\n",
        "            return lm\n",
        "\n",
        "        lm = get_landmark(filepath)\n",
        "\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        y = np.flipud(x) * [-1, 1]\n",
        "        c = eye_avg + eye_to_mouth * 0.1\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "\n",
        "        # read image\n",
        "        img = PIL.Image.open(filepath)\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh0c2zpjuDei"
      },
      "source": [
        "# Primary Application\n",
        "\n",
        "## About this notebook:\n",
        "This notebook performs manipulations in the $\\mathcal{W}+$ latent space of StyleGAN. An index is filled with precomputed CLIP embeddings that are searched with a dot-product. The top-*k* results are selected for the appropriate weighted average in latent space to result in an embedding vector $w$. Gradient decent is then performed directly on $w \\in \\mathcal{W}+$ to fine-tune the output.\n",
        "\n",
        "This notebook is compatable with both CPU and GPU Colab instances. The index makes inference on CPU reasonably viable, often converging to a satifactory output in less than 20 iterations of fine-tuning. GPU instances gain the benefit of a custom CUDA kernels developed by NVIDIA that significantly speeds up the fine-tuning stage at the cost of determinism. GPU instances often will be more useful than CPU instances, but if determinism and reproduciblity is an issue, use CPU instances.\n",
        "\n",
        "## Tips for prompt engineering:\n",
        "\n",
        "### Prompt Augmentation\n",
        "This notebook uses [exrex](https://github.com/asciimoo/exrex) to generate many strings from an input regex. Note that if a tilde (`~`) is found anywhere in an output string, the feature vector will be complemented (as to form an explicit NOT).\n",
        "\n",
        "<figure>\n",
        "    <dl>\n",
        "        <dt>\n",
        "            <code>The (quick ){0,1}brown fox jumps over the lazy (dog|~cat)</code>\n",
        "        </dt>\n",
        "        <dd>\n",
        "            <pre><code>The brown fox jumps over the lazy dog<br/><s>The brown fox jumps over the lazy cat</s><br/>The quick brown fox jumps over the lazy dog<br/><s>The quick brown fox jumps over the lazy cat</s></code></pre>\n",
        "        </dd>\n",
        "        <dt>\n",
        "            <code>dog + wolf</code>\n",
        "        </dt>\n",
        "        <dd>\n",
        "            <pre><code>dog  wolf\n",
        "dog   wolf<br/>dog    wolf<br/>dog     wolf<br/>dog      wolf<br/>dog       wolf<br/>...</code></pre>\n",
        "        </dd>\n",
        "            <code>dog \\+ wolf</code>\n",
        "        </dt>\n",
        "        <dd>\n",
        "            <pre><code>dog + wolf</code></pre>\n",
        "        </dd>\n",
        "    </dl>\n",
        "    <figcaption>Examples of regex prompting (and its pitfalls).</figcaption>\n",
        "</figure>\n",
        "\n",
        "The embeddings of each of these strings are then fused in some way so that it can be compared to a single image embedding (this notebook uses the formulation presented in [*Spherical Averages and Applications to Spherical Splines and Interpolation*](http://math.ucsd.edu/~sbuss/ResearchWeb/spheremean) for computing a weighted average on a sphere).\n",
        "\n",
        "This allows the same conceptual prompt to be input with different phrasings (which I postulate leads to better abstraction of concepts by CLIP in difficult scenarios) or seperate concepts to be explicitly combined.\n",
        "\n",
        "*Think of this as an extra tool in your toolkit. You don't always need it, but I find it quite handy.*\n",
        "\n",
        "#### Prompt Weighting\n",
        "Prompts may also be weighted by prepending a tag of the form <kbd>&lt;<i>float</i>&gt;</kbd> to a prompt. Weights are implicitly normalized to sum to one. The aformentioned tilde is a more flexible shorthand for prepending the prompt with `<-1>`.\n",
        "\n",
        "#### Image Prompting\n",
        "This notebook contains a system for prompting with images. Image embeddings are substituted for all entries that match a filename in <samp>./prompt_images</samp> verbatim. For instance, if <samp>dog.jpg</samp> exists in <samp>./prompt_images</samp>, then all prompts exactly matching `dog.jpg` will be substituted with an embedding generated from <samp>dog.jpg</samp>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzNZQVAP_lCc",
        "cellView": "form"
      },
      "source": [
        "#@markdown Input prompt (a valid regex):\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown Continue optimization on the current prompt:\n",
        "continue_opt = False #@param {type:\"boolean\"}\n",
        "#@markdown Lock prompt when `continue_opt` is enabled:\n",
        "prompt_lock = True #@param {type:\"boolean\"}\n",
        "#@markdown Show a preview of the generated strings passed to CLIP:\n",
        "display_prompt_preview = True #@param {type:\"boolean\"}\n",
        "#@markdown Cache final forward pass for continuation:\n",
        "cache_final = True #@param {type:\"boolean\"}\n",
        "#@markdown Number of backpropagation iterations:\n",
        "iterations =  15#@param {type:\"number\"}\n",
        "#@markdown Number of vectors to consider during initial lookup: <br>(recomended to be no larger than 4096)\n",
        "k =  18#@param {type:\"number\"}\n",
        "\n",
        "if not continue_opt or not prompt_lock:\n",
        "    aug_prompt_info = prompt_parse(prompt)\n",
        "    \n",
        "# Optionally print out the generated strings, printing explicitly negative strings red\n",
        "if display_prompt_preview:\n",
        "    prompt_preview(aug_prompt_info)\n",
        "\n",
        "with torch.no_grad():\n",
        "    if not continue_opt or not prompt_lock:\n",
        "        text_features = aug_prompts_to_features(aug_prompt_info, condense=True)\n",
        "        #text_features = rot@text_features.squeeze()\n",
        "        text_features = text_features.to(device)\n",
        "\n",
        "    if not continue_opt:\n",
        "        # Use the index if it exists, fallback on w_avg if not\n",
        "        if os.path.exists(index_path) and k!=0:\n",
        "            found_w = sample_index(text_features, mode='w').to(device)-G.mapping.w_avg\n",
        "        else:\n",
        "            found_w = torch.zeros(1,G.num_ws,G.w_dim).to(device)\n",
        "\n",
        "        # Prepare for gradient decent\n",
        "        found_w.requires_grad = True\n",
        "\n",
        "if not continue_opt:\n",
        "    #optimizer = torch.optim.SGD((found_w,),5)\n",
        "    optimizer = torch.optim.AdamW((found_w,),0.02,betas=(0.5,0.999))\n",
        "\n",
        "progress = tqdm(total=iterations)\n",
        "for i in range(iterations+1):\n",
        "    if (i!=0 or not continue_opt) or not cache_final:\n",
        "        optimizer.zero_grad()\n",
        "        with (torch.no_grad() if (not cache_final and i!=iterations) else contextlib.nullcontext()):\n",
        "            img = G.synthesis(found_w+G.mapping.w_avg, noise_mode='const', force_fp32=not cuda_available)\n",
        "            display(PIL.Image.fromarray((img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy(), 'RGB').resize((256,256)))\n",
        "    if i!=iterations:\n",
        "        img = stylegan_preprocess(img)\n",
        "        image_features = model.encode_image(img)\n",
        "        loss = -torch.nn.functional.cosine_similarity(image_features,text_features)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        progress.update()\n",
        "\n",
        "display(PIL.Image.fromarray((img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)[0].cpu().numpy(), 'RGB'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTI8SzEaUUIs"
      },
      "source": [
        "# Alpha: Generate index file for arbitrary models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r76k8fVUTV8"
      },
      "source": [
        "n_vecs = 10000\n",
        "\n",
        "# Adapted preprocessing routine for connecting StyleGAN to CLIP\n",
        "stylegan_transform = Compose([\n",
        "        Resize(model.visual.input_resolution),\n",
        "        lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "\n",
        "# Import existing index if found, otherwise initialise a new one.\n",
        "try:\n",
        "    vec_list = torch.from_numpy(np.load(index_path)).to(device)\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        os.mkdir(os.path.dirname(index_path))\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "    finally:\n",
        "        vec_list = torch.empty(0,model.visual.output_dim,dtype=(torch.float16 if cuda_available else torch.float32)).to(device)\n",
        "\n",
        "# Generate index\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        if hasattr(G.synthesis, 'input'):\n",
        "            m = make_transform((0,0), 0)\n",
        "            m = np.linalg.inv(m)\n",
        "            G.synthesis.input.transform.copy_(torch.from_numpy(m))\n",
        "            shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n",
        "            G.synthesis.input.affine.bias.data=shift.squeeze(0)\n",
        "            G.synthesis.input.affine.weight.data.zero_()\n",
        "        for i, seed in enumerate(tqdm(range(vec_list.shape[0],n_vecs), \n",
        "                                      total=len(range(vec_list.shape[0],n_vecs)))):\n",
        "            z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n",
        "            img = G(z, None, truncation_psi=1.0, noise_mode='const', force_fp32=not cuda_available)\n",
        "            img = stylegan_transform(img)\n",
        "            image_vec = model.encode_image(img)\n",
        "            vec_list = torch.cat([vec_list, image_vec],dim=0)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "# Export index\n",
        "np.save(index_path,vec_list.cpu().detach().float().numpy())\n",
        "print(f\"sha256: {sha256_file(index_path)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIE9OvqlBM-J"
      },
      "source": [
        "# Import exported index manually (for testing)\n",
        "if os.path.exists(index_path):\n",
        "    CLIP_vecs = torch.from_numpy(np.load(index_path))\n",
        "    seeded_z = torch.from_numpy(np.stack([np.random.RandomState(seed).randn(G.w_dim) for seed in range(CLIP_vecs.shape[0])]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JytSQK_bh19"
      },
      "source": [
        "# Prototypes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUuiwFdfblsR"
      },
      "source": [
        "# Rotation Analogies\n",
        "x = aug_prompts_to_features(prompt_parse(\"source\"), condense=True)\n",
        "y = aug_prompts_to_features(prompt_parse(\"destination\"), condense=True)\n",
        "z = aug_prompts_to_features(prompt_parse(\"input\"), condense=True)\n",
        "x = x/torch.norm(x)\n",
        "y = y/torch.norm(y)\n",
        "z = z/torch.norm(z)\n",
        "\n",
        "# Generate rotation matrix\n",
        "rot = torch.eye(x.shape[0])\n",
        "for _ in range(100):\n",
        "    outer = torch.outer(y,rot@x)-torch.outer(rot@x,y)\n",
        "    rot = (torch.eye(x.shape[0])+outer+(outer*outer)/(1+torch.dot(rot@x,y)))@rot\n",
        "\n",
        "rotated = (rot@x)\n",
        "\n",
        "print(torch.nn.functional.cosine_similarity(x.unsqueeze(0),x.unsqueeze(0)), torch.nn.functional.cosine_similarity(x.unsqueeze(0),y.unsqueeze(0)))\n",
        "print(torch.nn.functional.cosine_similarity(rotated.unsqueeze(0),x.unsqueeze(0)), torch.nn.functional.cosine_similarity(rotated.unsqueeze(0),y.unsqueeze(0)))\n",
        "print(torch.dot(z,rot@z))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}