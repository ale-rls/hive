{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference_playground.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuviq3qQkUFy"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'hyperstyle'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ6XEmlHlXbk",
        "outputId": "939783ec-ee6b-414b-f930-ad619b19bddd"
      },
      "source": [
        "!git clone https://github.com/yuval-alaluf/hyperstyle.git $CODE_DIR"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hyperstyle'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (200/200), done.\u001b[K\n",
            "remote: Compressing objects: 100% (177/177), done.\u001b[K\n",
            "remote: Total 200 (delta 32), reused 178 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (200/200), 52.67 MiB | 45.86 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaRUFuVHkzye",
        "outputId": "bc4d2728-33af-4274-adea-b424d80cfe5a"
      },
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-02 14:10:49--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211202T141023Z&X-Amz-Expires=300&X-Amz-Signature=3b7964a02e72e978db99678caf834a7e3c4d7e8a251e1d8c7baa0ca1cbe9b113&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-12-02 14:10:49--  https://github-releases.githubusercontent.com/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20211202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20211202T141023Z&X-Amz-Expires=300&X-Amz-Signature=3b7964a02e72e978db99678caf834a7e3c4d7e8a251e1d8c7baa0ca1cbe9b113&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.110.154, 185.199.109.154, 185.199.108.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.110.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "\rninja-linux.zip       0%[                    ]       0  --.-KB/s               \rninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-12-02 14:10:49 (5.83 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23baccYQlU9E"
      },
      "source": [
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d13v7In0kTJn"
      },
      "source": [
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from models.stylegan2.model import Generator\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_inversion\n",
        "from utils.domain_adaptation_utils import run_domain_adaptation\n",
        "from utils.model_utils import load_model, load_generator\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRjtz6uLkTJs"
      },
      "source": [
        "## Step 1: Select Domain\n",
        "Select which domain you wish to perform inference on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XESWAO65kTJt"
      },
      "source": [
        "#@title Select which domain you wish to perform inference on: { run: \"auto\" }\n",
        "experiment_type = 'afhq_wild' #@param ['faces', 'cars', 'afhq_wild']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4etDz82xkTJz"
      },
      "source": [
        "## Step 2: Prepare to Download Pretrained Models \n",
        "As part of this repository, we provide pretrained models for each of the above domains. Here, we'll create the download command needed for downloading the desired model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSnjlBZOkTJ0"
      },
      "source": [
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    command = f\"gdown --id {file_id} -O {save_path}/{file_name}\"\n",
        "    return command    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4sjldFMkTJ5"
      },
      "source": [
        "MODEL_PATHS = {\n",
        "    \"faces\": {\"id\": \"1C3dEIIH1y8w1-zQMCyx7rDF0ndswSXh4\", \"name\": \"hyperstyle_ffhq.pt\"},\n",
        "    \"cars\": {\"id\": \"1WZ7iNv5ENmxXFn6dzPeue1jQGNp6Nr9d\", \"name\": \"hyperstyle_cars.pt\"},\n",
        "    \"afhq_wild\": {\"id\": \"1OMAKYRp3T6wzGr0s3887rQK-5XHlJ2gp\", \"name\": \"hyperstyle_afhq_wild.pt\"}\n",
        "}\n",
        "path = MODEL_PATHS[experiment_type]\n",
        "hyperstyle_download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gil_lQ4c6mGL"
      },
      "source": [
        "W_ENCODERS_PATHS = {\n",
        "    \"faces\": {\"id\": \"1M-hsL3W_cJKs77xM1mwq2e9-J0_m7rHP\", \"name\": \"faces_w_encoder.pt\"},\n",
        "    \"cars\": {\"id\": \"1GZke8pfXMSZM9mfT-AbP1Csyddf5fas7\", \"name\": \"cars_w_encoder.pt\"},\n",
        "    \"afhq_wild\": {\"id\": \"1MhEHGgkTpnTanIwuHYv46i6MJeet2Nlr\", \"name\": \"afhq_wild_w_encoder.pt\"}\n",
        "}\n",
        "path = W_ENCODERS_PATHS[experiment_type]\n",
        "w_encoder_download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tozsg81kTKA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 3: Define Inference Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhyc7RqkTKB"
      },
      "source": [
        "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on.  \n",
        "While we provide default values to run this script, feel free to change as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kE5y1-skTKC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"faces\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"cars\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"afhq_wild\": {\n",
        "        \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
        "        \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
        "        \"image_path\": \"./notebooks/images/afhq_wild_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"domain_adaptation\": {  # used in a later part of the notebook, checkpoint path will be defined separately\n",
        "        \"image_path\": \"./notebooks/images/domain_adaptation.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzUHoD9ukTKG"
      },
      "source": [
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmOsCJWB6mGM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.  \n",
        "We'll download the model for the selected experiment and save it to the folder `../pretrained_models`.\n",
        "\n",
        "We also need to verify that the model was downloaded correctly. All of our models should weigh approximately 1.3GB.\n",
        "Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive. In that case, you should try downloading the model again after a few hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "jQ31J_m7kTJ8",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "76209d4d-57a7-41f4-9b6a-6789001c02be"
      },
      "source": [
        "!gdown --id 1C3dEIIH1y8w1-zQMCyx7rDF0ndswSXh4 -O /content/hyperstyle/pretrained_models/hyperstyle_ffhq.pt\n",
        "!gdown --id 1OMAKYRp3T6wzGr0s3887rQK-5XHlJ2gp -O /content/hyperstyle/pretrained_models/hyperstyle_afhq_wild.pt\n",
        "\n",
        "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "    print(f'Downloading HyperStyle model for {experiment_type}...')\n",
        "    os.system(hyperstyle_download_command)\n",
        "    print(hyperstyle_download_command)\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print(f'HyperStyle model for {experiment_type} already exists!')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1C3dEIIH1y8w1-zQMCyx7rDF0ndswSXh4\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n",
            "Permission denied: https://drive.google.com/uc?id=1OMAKYRp3T6wzGr0s3887rQK-5XHlJ2gp\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n",
            "Downloading HyperStyle model for afhq_wild...\n",
            "gdown --id 1OMAKYRp3T6wzGr0s3887rQK-5XHlJ2gp -O /content/hyperstyle/pretrained_models/hyperstyle_afhq_wild.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7fdf373d7321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperstyle_download_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPERIMENT_ARGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pretrained model was unable to be downloaded correctly!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pretrained_models/hyperstyle_afhq_wild.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUgVDc2s6mGN"
      },
      "source": [
        "In addition, we need to download the WEncoder for the desired domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKeTTMU26mGN"
      },
      "source": [
        "if not os.path.exists(EXPERIMENT_ARGS['w_encoder_path']) or os.path.getsize(EXPERIMENT_ARGS['w_encoder_path']) < 1000000:\n",
        "    print(f'Downloading the WEncoder model for {experiment_type}...')\n",
        "    os.system(w_encoder_download_command)\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(EXPERIMENT_ARGS['w_encoder_path']) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print(f'WEncoder model for {experiment_type} already exists!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAWrUehTkTKJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 4: Load Pretrained Model\n",
        "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-AOhP1kTKJ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
        "print('Model successfully loaded!')\n",
        "pprint.pprint(vars(opts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4weLFoPbkTKZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Step 5: Visualize Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2H9zFLJkTKa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n",
        "original_image = Image.open(image_path).convert(\"RGB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbLKtl-kTKc"
      },
      "source": [
        "if experiment_type == 'cars':\n",
        "    original_image = original_image.resize((192, 256))\n",
        "else:\n",
        "    original_image = original_image.resize((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPppA3wv6mGO",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "original_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oqf8JwzK0K",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Align Image\n",
        "\n",
        "Note: in this notebook we'll run alignment on the input image when working on the human facial domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ9Ce1aYzmFF"
      },
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from scripts.align_faces_parallel import align_face\n",
        "    if not os.path.exists(\"shape_predictor_68_face_landmarks.dat\"):\n",
        "        print('Downloading files for aligning face image...')\n",
        "        os.system('wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        os.system('bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2')\n",
        "        print('Done.')\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTZcKMdK8y77"
      },
      "source": [
        "input_is_aligned = False\n",
        "if experiment_type == \"faces\" and not input_is_aligned:\n",
        "    input_image = run_alignment(image_path)\n",
        "else:\n",
        "    input_image = original_image\n",
        "\n",
        "input_image.resize((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0BmXzu1kTKg"
      },
      "source": [
        "## Step 6: Perform Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3h3E7VLkTKg"
      },
      "source": [
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5eWR2S4OSDM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we'll run inference. By default, we'll run using 5 inference steps. You can change the parameter in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct_jm0obOSDM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "opts.n_iters_per_batch = 5\n",
        "opts.resize_outputs = False  # generate outputs at full resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls5zb0fRkTKs"
      },
      "source": [
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    result_batch, result_latents, _ = run_inversion(transformed_image.unsqueeze(0).cuda(), \n",
        "                                                    net, \n",
        "                                                    opts,\n",
        "                                                    return_intermediate_results=True)\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq0dkSz6kTKv"
      },
      "source": [
        "### Visualize Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVR03XT_kTK0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We'll visualize the step-by-step outputs side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca5BtxdUOSDN"
      },
      "source": [
        "if opts.dataset_type == \"cars\":\n",
        "    resize_amount = (256, 192) if opts.resize_outputs else (512, 384)\n",
        "else:\n",
        "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdR51hOROSDN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def get_coupled_results(result_batch, transformed_image):\n",
        "    result_tensors = result_batch[0]  # there's one image in our batch\n",
        "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
        "    input_im = tensor2im(transformed_image).resize(resize_amount)\n",
        "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
        "    res = Image.fromarray(res)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSDCvtTMOSDN"
      },
      "source": [
        "Note that the step-by-step outputs are shown left-to-right with the original input on the right-hand side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb3raAKFOSDN"
      },
      "source": [
        "res = get_coupled_results(result_batch, transformed_image)\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaB7RN7cOSDN"
      },
      "source": [
        "# save image \n",
        "outputs_path = \"./outputs\"\n",
        "os.makedirs(outputs_path, exist_ok=True)\n",
        "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISEMFxmekTK7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Domain Adaptation\n",
        "\n",
        "In the paper, we show that the weight offsets predicted by HyperStyle over the FFHQ domain are also applicable on fine-tuned generators such as toonify and StyleGAN-NADA.\n",
        "\n",
        "\n",
        "We demonstrate this idea below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKbAFK7_OSDO"
      },
      "source": [
        "generator_type = 'toonify' #@param ['toonify', 'pixar', 'sketch', 'disney_princess']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8W2uTB96mGR"
      },
      "source": [
        "# download fine-tuned generator\n",
        "FINETUNED_MODELS = {\n",
        "    \"toonify\": {'id': '1r3XVCt_WYUKFZFxhNH-xO2dTtF6B5szu', 'name': 'toonify.pt'},\n",
        "    \"pixar\": {'id': '1trPW-To9L63x5gaXrbAIPkOU0q9f_h05', 'name': 'pixar.pt'},\n",
        "    \"sketch\": {'id': '1aHhzmxT7eD90txAN93zCl8o9CUVbMFnD', 'name': 'sketch.pt'},\n",
        "    \"disney_princess\": {'id': '1rXHZu4Vd0l_KCiCxGbwL9Xtka7n3S2NB', 'name': 'disney_princess.pt'}\n",
        "}\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS['domain_adaptation']\n",
        "path = FINETUNED_MODELS[generator_type]\n",
        "\n",
        "generator_path = os.path.join(\"./pretrained_models\", path['name'])\n",
        "\n",
        "if not os.path.exists(generator_path):\n",
        "    print(f'Downloading fine-tuned {generator_type} generator...')\n",
        "    download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
        "    os.system(download_command)\n",
        "    print('Done.')\n",
        "else:\n",
        "    print(f'Fine-tuned {generator_type} generator already exists!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3v0X3ZWkTK8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# load model\n",
        "fine_tuned_generator = load_generator(generator_path)\n",
        "print(f'Fine-tuned {generator_type} generator successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERkgQzGB90Bz"
      },
      "source": [
        "# load faces HyperStyle model - we assume the model is already downloaded from above\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[\"faces\"]\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
        "print('Model successfully loaded!')\n",
        "pprint.pprint(vars(opts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdhfpGpI6mGR",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# load ReStyle e4e:\n",
        "RESTYLE_E4E_MODELS = {'id': '1e2oXVeBPXMQoUoC_4TNwAWpOPpSEhE_e', 'name': 'restlye_e4e.pt'}\n",
        "\n",
        "restyle_e4e_path = os.path.join(\"./pretrained_models\", RESTYLE_E4E_MODELS['name'])\n",
        "\n",
        "if not os.path.exists(restyle_e4e_path):\n",
        "    print('Downloading ReStyle-e4e model...')\n",
        "    download_command = get_download_model_command(file_id=RESTYLE_E4E_MODELS[\"id\"], file_name=RESTYLE_E4E_MODELS[\"name\"])\n",
        "    os.system(download_command)\n",
        "    print('Done.')\n",
        "else:\n",
        "    print('ReStyle-e4e model already exists!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ1MTJ0b6mGR"
      },
      "source": [
        "# load restyle-e4e model\n",
        "restyle_e4e, restyle_e4e_opts = load_model(restyle_e4e_path, is_restyle_encoder=True)\n",
        "print(f'ReStyle-e4e model successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW-CJsuwOSDO"
      },
      "source": [
        "# load image. Note that uploaded images must be aligned first, example image is already aligned.\n",
        "image_path = EXPERIMENT_DATA_ARGS['domain_adaptation'][\"image_path\"]\n",
        "input_is_aligned = True\n",
        "if not input_is_aligned:\n",
        "    input_image = run_alignment(image_path)\n",
        "else:\n",
        "    input_image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "input_image.resize((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmPWPODaOSDP"
      },
      "source": [
        "# transform image\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMjTyMzOSDP"
      },
      "source": [
        "restyle_e4e_opts.n_iters_per_batch = 5\n",
        "restyle_e4e_opts.resize_outputs = False\n",
        "opts.n_iters_per_batch = 5\n",
        "opts.resize_outputs = False  # generate outputs at full resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o81i-MtOOSDQ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    result, _ = run_domain_adaptation(transformed_image.unsqueeze(0).cuda(), \n",
        "                                      net, \n",
        "                                      opts, \n",
        "                                      fine_tuned_generator, \n",
        "                                      restyle_e4e, \n",
        "                                      restyle_e4e_opts)\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCoL6QE56mGR"
      },
      "source": [
        "final_res = tensor2im(result[0]).resize(resize_amount)\n",
        "input_im = tensor2im(transformed_image).resize(resize_amount)\n",
        "res = np.concatenate([np.array(input_im), np.array(final_res)], axis=1)\n",
        "res = Image.fromarray(res)\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPP8Rpjk6mGS"
      },
      "source": [
        "# save image \n",
        "outputs_path = f\"./outputs/domain_adaptation/{generator_type}\"\n",
        "os.makedirs(outputs_path, exist_ok=True)\n",
        "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}