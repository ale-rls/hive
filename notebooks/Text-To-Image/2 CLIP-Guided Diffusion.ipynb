{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "CLIP-Guided Diffusion",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "By [Katherine Crowson](https://twitter.com/RiversHaveWings). It uses OpenAI's 256x256 unconditional [ImageNet diffusion model](https://github.com/openai/guided-diffusion) together with [CLIP](https://github.com/openai/CLIP) to connect text prompts with images.\n",
        "\n",
        "Modified by [Daniel Russell](https://github.com/russelldc) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.\n",
        "\n",
        "Creates images of 1024x1024px with a 4 x Superresolution step added by Thomash. Could be a little slower if turned on or run out of memory but it usually works.\n",
        "\n",
        "\n",
        "##### Example: *The [Fermi Paradox](https://en.wikipedia.org/wiki/Fermi_paradox)*\n",
        "\n",
        "[![The Fermi Paradox](https://pollinations.ai/ipfs/QmVzXi6oWygPuFKqYj3858E1nciXf8YrcM9EQURBEurYv5?filename=fermi.jpg)]()\n",
        "\n"
      ],
      "metadata": {
        "id": "1YwMUyt9LHG1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Text prompt\n",
        "text_input = 'the fermi paradox' #@param {type: \"string\"}\n",
        "\n",
        "# Perform 4x neural super-resolution (from 256x256px to 1024x124)\n",
        "super_resolution = True   #@param {type: \"boolean\"}\n",
        "\n",
        "output_path = \"/content/output\"\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Kc0d6_lfNdnl",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "XIqUfrmvLIhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Upscale images/video frames\n",
        "\n",
        "\n",
        "loaded_upscale_model = False\n",
        "\n",
        "def upscale(filepath):\n",
        "  global loaded_upscale_model\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd /content/Real-ESRGAN\n",
        "    # Set up the environment\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input $filepath --netscale 4 --outscale 4 --half --output $output_path\n",
        "  filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  !mv -v $filepath_out $filepath\n",
        "  %cd -"
      ],
      "outputs": [],
      "metadata": {
        "id": "sPAj50F3NRWL",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Check the GPU status\n",
        "\n",
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {
        "id": "qZ3rNuAWAewx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion\n",
        "!pip install lpips kornia datetime"
      ],
      "outputs": [],
      "metadata": {
        "id": "-_UVMZCIAq_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Download the diffusion model\n",
        "!sudo apt install aria2\n",
        "!aria2c -x 5 --auto-file-renaming=false 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'"
      ],
      "outputs": [],
      "metadata": {
        "id": "7zAqFEykBHDL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\n",
        "# Check the GPU status\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "!nvidia-smi"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from datetime import datetime\n",
        "import kornia.color as KC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "outputs": [],
      "metadata": {
        "id": "JmbrcrhpBPC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Define necessary functions\n",
        "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
        "\n",
        "def interp(t):\n",
        "    return 3 * t**2 - 2 * t ** 3\n",
        "\n",
        "def perlin(width, height, scale=10, device=None):\n",
        "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
        "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
        "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
        "    wx = 1 - interp(xs)\n",
        "    wy = 1 - interp(ys)\n",
        "    dots = 0\n",
        "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
        "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
        "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
        "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
        "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
        "\n",
        "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
        "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
        "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
        "    for i in range(1 if grayscale else 3):\n",
        "        scale = 2 ** len(octaves)\n",
        "        oct_width = width\n",
        "        oct_height = height\n",
        "        for oct in octaves:\n",
        "            p = perlin(oct_width, oct_height, scale, device)\n",
        "            out_array[i] += p * oct\n",
        "            scale //= 2\n",
        "            oct_width *= 2\n",
        "            oct_height *= 2\n",
        "    return torch.cat(out_array)\n",
        "\n",
        "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
        "    out = perlin_ms(octaves, width, height, grayscale)\n",
        "    if grayscale:\n",
        "        out = TF.resize(size=(side_x, side_y), img=out.unsqueeze(0))\n",
        "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
        "    else:\n",
        "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
        "        out = TF.resize(size=(side_x, side_y), img=out)\n",
        "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
        "\n",
        "    out = ImageOps.autocontrast(out)\n",
        "    return out"
      ],
      "outputs": [],
      "metadata": {
        "id": "YHOj78Yvx8jP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.reshape([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.reshape([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, skip_augs=False, grayscale_cuts=False):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.skip_augs = skip_augs\n",
        "        self.grayscale_cuts = grayscale_cuts\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=1.0),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = T.Pad(input.shape[2]//5, fill=0)(input)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "\n",
        "        cutouts = []\n",
        "        for ch in range(cutn):\n",
        "            if ch > cutn - cutn//4:\n",
        "                cutout = input.clone()\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
        "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "\n",
        "            if self.grayscale_cuts:\n",
        "                cutout = KC.rgb_to_grayscale(cutout)\n",
        "                cutout = KC.grayscale_to_rgb(cutout)\n",
        "\n",
        "            if not self.skip_augs:\n",
        "                cutout = self.augs(cutout)\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "            del cutout\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def unitwise_norm(x):\n",
        "    if len(x.squeeze().shape) <= 1:\n",
        "        dim = None\n",
        "        keepdim = False\n",
        "    elif len(x.shape) in (2, 3):\n",
        "        dim = 1\n",
        "        keepdim = True\n",
        "    elif len(x.shape) == 4:\n",
        "        dim = (1, 2, 3)\n",
        "        keepdim = True\n",
        "    else:\n",
        "        raise ValueError(f'got a parameter with shape not in (1, 2, 3, 4) {x}')\n",
        "    return x.norm(dim = dim, keepdim = keepdim, p = 2)\n",
        "\n",
        "\n",
        "def adaptive_clip_grad(parameters, clipping = 0.01, eps = 1e-3):\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    if len(parameters) == 0:\n",
        "        return\n",
        "    for p in parameters:\n",
        "        param_norm = unitwise_norm(p).clamp_(min = eps)\n",
        "        grad_norm = unitwise_norm(p.grad)\n",
        "        max_norm = param_norm * clipping\n",
        "        trigger = grad_norm > max_norm\n",
        "        clipped_grad = p.grad * (max_norm / grad_norm.clamp(min = 1e-6))\n",
        "        new_grads = torch.where(trigger, clipped_grad, p.grad)\n",
        "        p.grad.detach().copy_(new_grads)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def do_run():\n",
        "    loss_values = []\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs, grayscale_cuts=grayscale_cuts)\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in text_prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "    \n",
        "    if perlin_init:\n",
        "        if perlin_mode == 'color':\n",
        "            init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "            init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "        elif perlin_mode == 'gray':\n",
        "           init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "           init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "        else:\n",
        "           init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "           init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "        \n",
        "        # init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device)\n",
        "        init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "        del init2\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, y=None):\n",
        "        with torch.enable_grad():\n",
        "            x = x.detach().requires_grad_()\n",
        "            n = x.shape[0]\n",
        "            my_t = torch.ones([n], device=device, dtype=torch.long) * cur_t\n",
        "            out = diffusion.p_mean_variance(model, x, my_t, clip_denoised=False, model_kwargs={'y': y})\n",
        "            fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "            x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "            image_embeds = clip_model.encode_image(clip_in).float()\n",
        "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "            dists = dists.view([cutn, n, -1])\n",
        "            losses = dists.mul(weights).sum(2).mean(0)\n",
        "\n",
        "            loss_values.append(losses.sum().item())\n",
        "\n",
        "            tv_losses = tv_loss(x_in)\n",
        "            range_losses = range_loss(out['pred_xstart'])\n",
        "            loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "            if init is not None and init_scale:\n",
        "                init_losses = lpips_model(x_in, init)\n",
        "                loss = loss + init_losses.sum() * init_scale\n",
        "\n",
        "            # if clip_grad and int(timestep_respacing) - cur_t < int(timestep_respacing)//4:\n",
        "            if clip_grad:\n",
        "                adaptive_clip_grad([x])\n",
        "            return -1 * torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=clip_denoised,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=randomize_class,\n",
        "        )\n",
        "        \n",
        "        for j, sample in enumerate(samples):\n",
        "            display.clear_output(wait=True)\n",
        "            cur_t -= 1\n",
        "            if j % 1 == 0 or cur_t == -1:\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    if j % display_rate == 0:\n",
        "                      tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                      current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n",
        "                      filename = f'{output_path}/progress_{i * batch_size + k + j:05}.jpg'\n",
        "                      TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                      if super_resolution:\n",
        "                        upscale(filename)\n",
        "                      #display.display(display.Image(filename))\n",
        "\n",
        "        plt.plot(np.array(loss_values), 'r')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# timestep_respacing = '25' # Modify this value to decrease the number of timesteps.\n",
        "timestep_respacing = 'ddim100' # Modify this value to decrease the number of timesteps.\n",
        "diffusion_steps = 1000\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': diffusion_steps,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': timestep_respacing,\n",
        "    'image_size': 256,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "})\n",
        "side_x = side_y = model_config['image_size']\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load('256x256_diffusion_uncond.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fpbody2NCR7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VnQjGugaDZPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings for this run:"
      ],
      "metadata": {
        "id": "9zY-8I90LkC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "text_prompts = text_input.split(\"|\")\n",
        "\n",
        "image_prompts = [\n",
        "    # 'mona.jpg',\n",
        "]\n",
        "\n",
        "# 350/50/50/32 and 500/0/0/64 have worked well for 25 timesteps on 256px\n",
        "# Also, sometimes 1 cutn actually works out fine\n",
        "\n",
        "clip_guidance_scale = 1000 # 1000 - Controls how much the image should look like the prompt.\n",
        "tv_scale = 0 # 150 - Controls the smoothness of the final output.\n",
        "range_scale = 0 # 50 - Controls how far out of range RGB values are allowed to be.\n",
        "cutn = 32 # 16 - Controls how many crops to take from the image.\n",
        "\n",
        "init_image = None # None - URL or local path\n",
        "init_scale = 0 # 0 - This enhances the effect of the init image, a good value is 1000\n",
        "skip_timesteps = 0 # 0 - Controls the starting point along the diffusion timesteps\n",
        "perlin_init = False # False - Option to start with random perlin noise\n",
        "perlin_mode = 'mixed' # 'mixed' ('gray', 'color')\n",
        "\n",
        "skip_augs = False # False - Controls whether to skip torchvision augmentations\n",
        "grayscale_cuts = False # False - Controls whether CLIP discriminates a grayscale version of the image\n",
        "randomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\n",
        "clip_denoised = True # False - Determines whether CLIP discriminates a noisy or denoised image\n",
        "\n",
        "clip_grad = True # False - Experimental: Using adaptive clip grad in the cond_fn\n",
        "\n",
        "seed = None"
      ],
      "outputs": [],
      "metadata": {
        "id": "U0PwzFZbLfcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually do the run..."
      ],
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "seed = random.randint(0, 2**32) # Choose a random seed and print it for reproduction\n",
        "print('seed:', seed)\n",
        "\n",
        "display_rate = 1\n",
        "n_batches = 1 # 1 - Controls how many consecutive batches of images are generated\n",
        "batch_size = 1 # 1 - Controls how many images are generated in parallel in a batch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "try:\n",
        "    do_run()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "out_file=output_path+\"/video.mp4\"\n",
        "\n",
        "!mkdir -p /tmp/ffmpeg\n",
        "!cp $output_path/*.jpg /tmp/ffmpeg\n",
        "last_frame=!ls -t /tmp/ffmpeg/*.jpg | head -1\n",
        "last_frame = last_frame[0]\n",
        "\n",
        "# Copy last frame to start and duplicate at end so it sticks around longer\n",
        "end_still_seconds = 4\n",
        "!cp -v $last_frame /tmp/ffmpeg/0000.jpg\n",
        "for i in range(end_still_seconds * 10):\n",
        "  pad_file = f\"/tmp/ffmpeg/zzzz_pad_{i:05}.jpg\"\n",
        "  !cp -v $last_frame $pad_file\n",
        "\n",
        "!ffmpeg  -r 10 -i /tmp/ffmpeg/%*.jpg -y -c:v libx264 /tmp/vid_no_audio.mp4\n",
        "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n",
        "\n",
        "print(\"Written\", out_file)\n",
        "!sleep 2\n",
        "!rm -r /tmp/ffmpeg"
      ],
      "outputs": [],
      "metadata": {
        "id": "jLqi7rRGOmS8"
      }
    }
  ]
}