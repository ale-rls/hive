{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward VQGAN CLIP - Using a pretrained model .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/hive/blob/notebook-esrgan/Feed_Forward_VQGAN_CLIP_Using_a_pretrained_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHN09TAisnpy"
      },
      "source": [
        "# Feed Forward VQGAN_CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BIfsw25sk18"
      },
      "source": [
        "\n",
        "Feed forward VQGAN-CLIP model, where the goal is to eliminate the need for optimizing the latent space of VQGAN for each input prompt. This is done by training a model that takes as input a text prompt, and returns as an output the VQGAN latent space, which is then transformed into an RGB image. The model is trained on a dataset of text prompts and can be used on unseen text prompts. The loss function is minimizing the distance between the CLIP generated image features and the CLIP input text features. Additionally, a diversity loss can be used to make increase the diversity of the generated images given the same prompt.\n",
        "\n",
        "This notebooks shows how to use a pre-trained model for generating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVOQrr6OSijT"
      },
      "source": [
        "super_resolution = True\n",
        "output_path = \"/content/output\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAJf6UVYS3Mz"
      },
      "source": [
        "!mkdir -p $output_path\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LreFUFSkl9"
      },
      "source": [
        "#@title Upscale images/video frames\n",
        "\n",
        "!sudo apt install aria2\n",
        "\n",
        "loaded_upscale_model = False\n",
        "from os.path import dirname\n",
        "\n",
        "def upscale(filepath, delete_original=True):\n",
        "  if not super_resolution:\n",
        "    return\n",
        "  global loaded_upscale_model\n",
        "  if not loaded_upscale_model:\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git /content/Real-ESRGAN\n",
        "    %cd /content/Real-ESRGAN\n",
        "    # Set up the environment\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    if not Path(\"cc12m_32x1024.th\").exists():\n",
        "      !aria2c -x 5 --auto-file-renaming=false 'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth' -o experiments/pretrained_models/cc12m_32x1024.th\n",
        "\n",
        "    #!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "    %cd -\n",
        "    loaded_upscale_model = True \n",
        "  \n",
        "  %cd /content/Real-ESRGAN\n",
        "  !python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth --input $filepath --netscale 4 --outscale 4 --half --output $output_path --suffix upscaled\n",
        "  dir = dirname(filepath)\n",
        "  !find $dir | grep .jpg | grep -v upscaled |xargs -i rm {}\n",
        "  #filepath_out = filepath.replace(\".jpg\",\"_out.jpg\")\n",
        "  #if delete_original:\n",
        "  %cd -\n",
        "upscale(\"/content/output\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ_IytBXfLGq"
      },
      "source": [
        "from os.path import dirname\n",
        "dir = dirname(\"/content/output/bla.jpg\")\n",
        "!find $dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-fhPhpddLOH"
      },
      "source": [
        "upscale(\"/content/output\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eto369VFWSMg"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/mehdidc/feed_forward_vqgan_clip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsAT4IfWWZig"
      },
      "source": [
        "cd feed_forward_vqgan_clip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNPGieSMWrex"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOXgso54Wi6A"
      },
      "source": [
        "!sudo apt install aria2\n",
        "from pathlib import Path\n",
        "if not Path(\"vqgan_imagenet_f16_16384.yaml\").exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false 'https://github.com/mehdidc/feed_forward_vqgan_clip/releases/download/0.1/vqgan_imagenet_f16_16384.yaml' -o vqgan_imagenet_f16_16384.yaml\n",
        "    !aria2c -x 5 --auto-file-renaming=false 'https://github.com/mehdidc/feed_forward_vqgan_clip/releases/download/0.1/vqgan_imagenet_f16_16384.ckpt' -o vqgan_imagenet_f16_16384.ckpt\n",
        "\n",
        "\n",
        "if not Path(\"cc12m_32x1024.th\").exists():\n",
        "    !aria2c -x 5 --auto-file-renaming=false 'https://github.com/mehdidc/feed_forward_vqgan_clip/releases/download/0.1/cc12m_32x1024.th' -o cc12m_32x1024.th\n",
        "#check available models at https://github.com/mehdidc/feed_forward_vqgan_clip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYS1xi4ks5as"
      },
      "source": [
        "# Load model\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKlH2a90X2vs"
      },
      "source": [
        "from IPython.display import Image\n",
        "import torch\n",
        "import clip\n",
        "from main import load_vqgan_model, CLIP_DIM, clamp_with_grad, synth\n",
        "import torchvision\n",
        "\n",
        "model_path = \"cc12m_32x1024.th\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "net = torch.load(model_path, map_location=\"cpu\").to(device)\n",
        "config = net.config\n",
        "vqgan_config = config.vqgan_config \n",
        "vqgan_checkpoint = config.vqgan_checkpoint\n",
        "clip_model = config.clip_model\n",
        "clip_dim = CLIP_DIM\n",
        "perceptor = clip.load(clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
        "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPyPr0s-nZ-h"
      },
      "source": [
        "# Generation of images from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTn51eBQXkUX"
      },
      "source": [
        "texts = [\n",
        "    \"berghain queue\"\n",
        "]\n",
        "toks = clip.tokenize(texts, truncate=True)\n",
        "H = perceptor.encode_text(toks.to(device)).float()\n",
        "with torch.no_grad():\n",
        "    z = net(H)\n",
        "    z = clamp_with_grad(z, z_min.min(), z_max.max())\n",
        "    xr = synth(model, z)\n",
        "grid = torchvision.utils.make_grid(xr.cpu(), nrow=len(xr))\n",
        "out_path = f\"{output_path}/gen.jpg\"\n",
        "upscale(out_path)\n",
        "torchvision.transforms.functional.to_pil_image(grid).save(out_path)\n",
        "sz = 256\n",
        "Image(out_path, width=sz*len(texts), height=sz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7M34dMnpIb"
      },
      "source": [
        "# Interpolation video from a set of text prompts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRPKtydsWjXz"
      },
      "source": [
        "from base64 import b64encode\n",
        "from IPython.display import HTML\n",
        "nb_interm = 32 # nb of intermediate images between each successive text prompts\n",
        "bs = 8 # reduce bs (batch size) if memory error\n",
        "texts = [\n",
        "  'fake shaman',\n",
        "  'authentic shaman',\n",
        "  'priest',\n",
        "  'sinner',\n",
        "]\n",
        "toks = clip.tokenize(texts, truncate=True)\n",
        "alpha = torch.linspace(0,1,nb_interm).view(-1,1).to(device)\n",
        "feats = perceptor.encode_text(toks.to(device)).float()\n",
        "\n",
        "H_list = []\n",
        "for i in range(len(texts)-1):\n",
        "  Hi = feats[i:i+1] * (1-alpha) + feats[i+1:i+2] * alpha\n",
        "  H_list.append(Hi)\n",
        "H = torch.cat(H_list)\n",
        "xr_list = []\n",
        "with torch.no_grad():\n",
        "  for i in range(0, len(H), bs):\n",
        "    z = net(H[i:i+bs])\n",
        "    z = clamp_with_grad(z, z_min.min(), z_max.max())\n",
        "    xr = synth(model, z)\n",
        "    xr_list.append(xr.cpu())\n",
        "xr = torch.cat(xr_list)\n",
        "grid = torchvision.utils.make_grid(xr.cpu(), nrow=len(xr))\n",
        "!rm -f *.jpg *.mp4\n",
        "out_path = f\"{output_path}/gen.png\"\n",
        "torchvision.transforms.functional.to_pil_image(grid).save(out_path)\n",
        "upscale(out_path)\n",
        "for i, img in enumerate(xr):\n",
        "  filepath = f\"{output_path}/image_{i:05d}.jpg\"\n",
        "  torchvision.transforms.functional.to_pil_image(img).save(filepath)\n",
        "  upscale(filepath)\n",
        "!ffmpeg -framerate 15 -pattern_type glob -i 'image*.jpg'  -c:v libx264 -r 30 -pix_fmt yuv420p video.mp4 1>&2 2>/dev/null\n",
        "# Show video\n",
        "mp4 = open(\"video.mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=256 height=256 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1LN_4OIQL2d"
      },
      "source": [
        "Image(\"gen.png\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}